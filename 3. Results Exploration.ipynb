{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:15.157163Z",
     "start_time": "2020-03-16T13:47:15.141781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:17.537619Z",
     "start_time": "2020-03-16T13:47:15.510549Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as albu\n",
    "import albumentations.pytorch as albu_pt\n",
    "from sklearn.metrics import jaccard_score\n",
    "%matplotlib inline\n",
    "\n",
    "import apex\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import pytorch_tools as pt\n",
    "\n",
    "from src.dataset import OpenCitiesDataset, OpenCitiesTestDataset, InriaTilesDataset, get_dataloaders\n",
    "from src.augmentations import get_aug\n",
    "from src.callbacks import ThrJaccardScore\n",
    "from pytorch_tools.fit_wrapper.callbacks import SegmCutmix\n",
    "from pytorch_tools.tta_wrapper import TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:17.557097Z",
     "start_time": "2020-03-16T13:47:17.539764Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from src.utils import MODEL_FROM_NAME\n",
    "from src.utils import TargetWrapper\n",
    "from pytorch_tools.fit_wrapper.callbacks import Callback\n",
    "from pytorch_tools.utils.misc import to_numpy\n",
    "from src.utils import criterion_from_list\n",
    "from src.utils import ToCudaLoader\n",
    "from src.dataset import get_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:17.586249Z",
     "start_time": "2020-03-16T13:47:17.558304Z"
    }
   },
   "outputs": [],
   "source": [
    "# BS = 32\n",
    "# train_dtld_gpu , val_dtld_gpu = get_dataloaders(\n",
    "#     [\n",
    "#         \"tier1\", \n",
    "# #         \"tier2\"\n",
    "#     ],\n",
    "#     batch_size=BS\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:17.600573Z",
     "start_time": "2020-03-16T13:47:17.587461Z"
    }
   },
   "outputs": [],
   "source": [
    "val_aug = get_aug(\"val\", size=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:17.837789Z",
     "start_time": "2020-03-16T13:47:17.601745Z"
    }
   },
   "outputs": [],
   "source": [
    "val_tier1 = OpenCitiesDataset(\n",
    "    split=\"val\",\n",
    "    transform=val_aug,\n",
    "    imgs_path=\"data/tier_1-images-512\",\n",
    "    masks_path=\"data/tier_1-masks-512\",\n",
    ")\n",
    "val_tier1_bo = OpenCitiesDataset(\n",
    "    split=\"val\",\n",
    "    transform=val_aug,\n",
    "    imgs_path=\"data/tier_1-images-512\",\n",
    "    masks_path=\"data/tier_1-masks-512\",\n",
    "    buildings_only=True,\n",
    ")\n",
    "val_tier2 = OpenCitiesDataset(\n",
    "    split=\"val\",\n",
    "    transform=val_aug,\n",
    "    imgs_path=\"data/tier_2-images-512\",\n",
    "    masks_path=\"data/tier_2-masks-512\",\n",
    ")\n",
    "test_dataset512 = OpenCitiesTestDataset(transform=get_aug(\"test\", size=512))\n",
    "test_dataset384 = OpenCitiesTestDataset(transform=get_aug(\"test\", size=384))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:17.860289Z",
     "start_time": "2020-03-16T13:47:17.839234Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_canny(image, sigma=0.33, fixed_thr=False):\n",
    "    if image.max() <= 1:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "#     image = cv2.blur(image, (11, 11))\n",
    "    is_image = False\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        is_image = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # compute the median of the single channel pixel intensities\n",
    "    v = np.median(image)\n",
    "    # apply automatic Canny edge detection using the computed median\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    if fixed_thr:\n",
    "        lower, upper = 160, 200\n",
    "    edged = cv2.Canny(image, lower, upper)\n",
    "    # return the edged image\n",
    "    if is_image:\n",
    "        return np.stack([edged] * 3, axis=2) / 255\n",
    "    else:\n",
    "        return edged / 255\n",
    "\n",
    "def remove_model_from_name(state_dict):\n",
    "    new_sd = {}\n",
    "    for k, v in state_dict:\n",
    "        new_sd[k[6:]] = v\n",
    "    return \n",
    "\n",
    "def process_adaptive_thr(pred):\n",
    "    thr_mask = cv2.threshold(\n",
    "        (pred * 255).astype(np.uint8),0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU\n",
    "    )[1] / 255\n",
    "    thr_mask[pred < 0.2] = 0\n",
    "    return thr_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:17.877103Z",
     "start_time": "2020-03-16T13:47:17.861489Z"
    }
   },
   "outputs": [],
   "source": [
    "PREV_WEIGHTS = None\n",
    "PREV_MODEL = None\n",
    "SINGLE_IMG = None\n",
    "SINGLE_MASK = None\n",
    "MEAN=(0.485, 0.456, 0.406)\n",
    "STD=(0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:17.891919Z",
     "start_time": "2020-03-16T13:47:17.878572Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASETS = {\"val_tier1\" : val_tier1, \"val_tier1_bo\": val_tier1_bo, \"val_tier2\" : val_tier2, \"test512\": test_dataset512, \"test384\": test_dataset384 }\n",
    "OFFSET = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:17.978623Z",
     "start_time": "2020-03-16T13:47:17.893108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07074a6208f74518848ef5f50c71c9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options=('val_tier1', 'val_tier1_bo', 'val_tier2', 'test…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    weights=sorted(os.listdir(\"logs/\")),\n",
    "    N=widgets.IntSlider(min=0, max=32, continuous_update=False),\n",
    "    offset=(0, 2000, 100),\n",
    "    thr=widgets.FloatSlider(0.5, min=0.2, max=0.8, step=0.1, continuous_update=False),\n",
    "    \n",
    ")\n",
    "def foo(\n",
    "    dataset=DATASETS.keys(),\n",
    "    weights=None, N=0, offset=0, thr=0.5, adaptive_thr=False, use_tta=False, \n",
    "    equilize=False, detect_edges=False, watershed=False, overlay_pred=False, overlay_true=False,\n",
    "    use_gmean=False,\n",
    "):\n",
    "    global PREV_WEIGHTS\n",
    "    global PREV_MODEL\n",
    "    global SINGLE_IMG\n",
    "    global SINGLE_PRED\n",
    "    \n",
    "    if weights is None:\n",
    "        print(\"select weights\")\n",
    "        return \n",
    "    \n",
    "    if weights != PREV_WEIGHTS:\n",
    "        PREV_WEIGHTS = weights\n",
    "        log_path = \"logs/\" + weights + \"/\"\n",
    "        config = yaml.load(open(log_path + \"config.yaml\"))\n",
    "        model = MODEL_FROM_NAME[config[\"segm_arch\"]](config[\"arch\"], **config.get(\"model_params\", {})).cuda()\n",
    "        state_dict = torch.load(log_path + 'model.chpn')[\"state_dict\"]\n",
    "        if \"model.\" in list(state_dict.keys())[0]:\n",
    "            # model was trained using tta need to remove `model` from name\n",
    "            state_dict = {k[6:]:v for k,v in state_dict.items()}\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model.eval()\n",
    "        PREV_MODEL = model\n",
    "\n",
    "    if \"test\" in dataset:\n",
    "        _, img, _ = DATASETS[dataset][N + offset]\n",
    "        mask = torch.zeros_like(img)\n",
    "    else:\n",
    "        img, mask = DATASETS[dataset][N + offset]\n",
    "    with torch.no_grad():\n",
    "        if use_tta:\n",
    "            model = pt.tta_wrapper.TTA(PREV_MODEL, segm=True, h_flip=True, rotation=[90], merge=\"gmean\" if use_gmean else \"mean\", activation=\"sigmoid\")\n",
    "        else:\n",
    "            model = PREV_MODEL\n",
    "        pred = model(img.view(1, *img.shape).cuda())\n",
    "        if not use_tta:\n",
    "            pred.sigmoid_()\n",
    "    \n",
    "    img = to_numpy(img).swapaxes(0, 2)\n",
    "    img = np.clip((img * STD + MEAN), 0, 1)\n",
    "    if equilize:\n",
    "        img = albu.Equalize(always_apply=True, by_channels=False)(image=(img * 255).astype(np.uint8))[\"image\"] / 255\n",
    "    \n",
    "    mask = to_numpy(mask).swapaxes(0, 2)\n",
    "    mask[:, :, :2] = (mask[:, :, :2] + 1) * 0.5\n",
    "    pred = to_numpy(pred.squeeze())\n",
    "    if detect_edges:\n",
    "        img = auto_canny(img, fixed_thr=True, sigma=0.5)\n",
    "        pred = auto_canny(pred, fixed_thr=True)\n",
    "    thr_mask = (pred > thr).astype(np.uint8)\n",
    "    if adaptive_thr:\n",
    "        thr_mask = process_adaptive_thr(pred)\n",
    "    \n",
    "    score = jaccard_score(mask[:,:,2], thr_mask.T, average=\"micro\")\n",
    "    if mask[:,:, 2].sum() == 0 and thr_mask.sum() == 0:\n",
    "        score = 1\n",
    "#     if watershed: \n",
    "#         img, pred = watershed(img, pred)\n",
    "    if overlay_pred:\n",
    "        img[thr_mask.T == 1] = [1, 0, 0]\n",
    "    if overlay_true:\n",
    "        img[mask[:, :, 2] == 1] = [0, 0, 1]\n",
    "        if overlay_pred:\n",
    "            img[(mask[:, :, 2] == 1) & (thr_mask.T == 1)] = [1, 0, 1]\n",
    "    pred = np.stack([pred.T] * 3, 2)\n",
    "    thr_mask = np.stack([thr_mask.T] * 3, 2)\n",
    "    stacked = np.hstack([img, mask, pred, thr_mask])\n",
    "    plt.figure(figsize=(32,8))\n",
    "    plt.imshow(stacked, cmap=\"gray\")\n",
    "    plt.title(f\"Jaccard={score:.3f}\", fontdict={\"fontsize\": 25})\n",
    "    plt.axis(\"off\")\n",
    "    SINGLE_IMG = pred\n",
    "    SINGLE_PRED = thr_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:18.782346Z",
     "start_time": "2020-03-16T13:47:18.640902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.bifpn_2l_2dtst_reduced_focal_20200303_202324\r\n",
      "10.bifpn_2l_3dtst_reduced_focal_20200303_205035\r\n",
      "11.bifpn_2l_2dtst_lookahead_cut_20200304_180726\r\n",
      "11.bifpn_2l_2dtst_seres101_20200304_233016\r\n",
      "11.bifpn_2l_2dtst_seres101_resume_20200305_111319\r\n",
      "12.bifpn_3l_1dtst_effnetb3_20200305_134355\r\n",
      "12.bifpn_3l_1dtst_seres50_tta_20200305_172219\r\n",
      "13.bifpn_3l_3dtst_effnetb3_resume_20200305_230828\r\n",
      "13.bifpn_4l_3dtst_effnetb5_hard_augs_20200306_133212\r\n",
      "13.bifpn_4l_3dtst_seres101_better_hard_augs_20200307_113601\r\n",
      "13.bifpn_4l_3dtst_seres101_hard_augs_20200307_113309\r\n",
      "13.deeplab_3dtst_seres101_hard_augs_20200311_202325\r\n",
      "13.test_bifpn_4l_3dtst_effnetb5_20200305_231118\r\n",
      "14.tune_2dtst_bifpn_4l_effnetb5_20200306_184111\r\n",
      "14.tune_2dtst_bifpn_4l_effnetb5_hard_augs_20200306_232835\r\n",
      "14.tune_2dtst_bifpn_4l_seres101_hard_augs_20200310_210420\r\n",
      "14.tune_2dtst_bifpn_4l_seres101_hard_augs_OS16_20200313_123409\r\n",
      "14.tune_2dtst_deeplab_seres101_hard_augs_20200312_113206\r\n",
      "15.tune_1dtst_bifpn_4l_effnetb5_hard_augs_buildings_only_20200311_001521\r\n",
      "15.tune_1dtst_bifpn_4l_seres101_hard_augs_buildings_only_20200311_101635\r\n",
      "15.tune_1dtst_bifpn_4l_seres101_hard_augs_OS16_20200314_122204\r\n",
      "15.tune_1dtst_deeplab_seres101_hard_augs_buildings_only_20200312_202257\r\n",
      "6.segm_bifpn_2l_dice_hinge_2dtst_20200227_210635\r\n",
      "8.bifpn_reduced_focal_20200302_191335\r\n",
      "8.deeplab_focal_20200302_183406\r\n",
      "8.deeplab_reduced_focal_20200302_183355\r\n",
      "8.deeplab_whinge_20200302_121157\r\n",
      "9.bifpn_3l_3dtst_dice_reduced_focal_20200302_210016\r\n",
      "9.bifpn_3l_3dtst_dice_whinge_20200302_205817\r\n"
     ]
    }
   ],
   "source": [
    "!ls logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:32.002880Z",
     "start_time": "2020-03-16T13:47:19.488574Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def load_from_path(path):\n",
    "    log_path = \"logs/\" + path + \"/\"\n",
    "    config = yaml.load(open(log_path + \"config.yaml\"))\n",
    "    model = MODEL_FROM_NAME[config[\"segm_arch\"]](config[\"arch\"], **config.get(\"model_params\", {})).cuda()\n",
    "    state_dict = torch.load(log_path + 'model.chpn')[\"state_dict\"]\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model = pt.tta_wrapper.TTA(\n",
    "        model.eval(), segm=True, h_flip=True, rotation=[90], merge=\"mean\", activation=\"sigmoid\"\n",
    "    )\n",
    "#     jit_model = torch.jit.trace(model, torch.ones((2, 3, 256, 256)).cuda())\n",
    "    apex_model = apex.amp.initialize(model, verbosity=False)\n",
    "    return apex_model\n",
    "\n",
    "best_models = [\n",
    "#     \"10.bifpn_2l_2dtst_reduced_focal_20200303_202324\",\n",
    "#     \"11.bifpn_2l_2dtst_lookahead_cut_20200304_180726\",\n",
    "    \"14.tune_2dtst_bifpn_4l_effnetb5_20200306_184111\",\n",
    "    \"14.tune_2dtst_bifpn_4l_effnetb5_hard_augs_20200306_232835\",\n",
    "    \"14.tune_2dtst_bifpn_4l_seres101_hard_augs_20200310_210420\",\n",
    "    \"14.tune_2dtst_bifpn_4l_seres101_hard_augs_OS16_20200313_123409\",\n",
    "    \"15.tune_1dtst_bifpn_4l_effnetb5_hard_augs_buildings_only_20200311_001521\",\n",
    "    \"15.tune_1dtst_bifpn_4l_seres101_hard_augs_buildings_only_20200311_101635\",\n",
    "    \"15.tune_1dtst_bifpn_4l_seres101_hard_augs_OS16_20200314_122204\",\n",
    "]\n",
    "# jit_model = load_from_path(best_models[0])\n",
    "loaded_best_models = list(map(load_from_path, best_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:32.029983Z",
     "start_time": "2020-03-16T13:47:32.004791Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_single_img(img, merge_by_mean=True, adaptive_thr=True, thr=0.5):\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def make_pred(model):\n",
    "        return to_numpy(model(img.view(1, *img.shape).cuda()).squeeze())\n",
    "    \n",
    "    preds = np.array([make_pred(model) for model in loaded_best_models])\n",
    "    pred = np.mean(preds, axis=0)\n",
    "    if merge_by_mean:\n",
    "        if adaptive_thr:\n",
    "            thr_mask = process_adaptive_thr(pred)\n",
    "        else:\n",
    "            thr_mask = (pred > thr).astype(np.uint8)\n",
    "    else:\n",
    "        if adaptive_thr:\n",
    "            thr_masks = [process_adaptive_thr(p) for p in preds]\n",
    "        else:\n",
    "            thr_masks = (preds > thr).astype(np.uint8)\n",
    "        thr_mask = np.median(thr_masks, axis=0).astype(np.uint8)\n",
    "    return pred, thr_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:33.424327Z",
     "start_time": "2020-03-16T13:47:32.031299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad6a91902c847949beabb3e79530869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options=('val_tier1', 'val_tier1_bo', 'val_tier2', 'test…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    N_model=(0, len(loaded_best_models), 1),\n",
    "    N=widgets.IntSlider(min=0 + OFFSET, max=32 + OFFSET, continuous_update=False),\n",
    "    thr=widgets.FloatSlider(0.5, min=0.2, max=0.8, step=0.1, continuous_update=False),\n",
    ")\n",
    "def foo(\n",
    "    dataset=DATASETS.keys(),\n",
    "    N_model=0,\n",
    "    N=0, \n",
    "    offset=(0, 4000, 100),\n",
    "    thr=0.5,\n",
    "    adaptive_thr=True,\n",
    "    overlay_pred=False, \n",
    "    overlay_true=False,\n",
    "    merge_by_mean=True,\n",
    "):\n",
    "    global SINGLE_PRED\n",
    "    N += offset\n",
    "    if \"test\" in dataset:\n",
    "        _, img, _ = DATASETS[dataset][N]\n",
    "        mask = torch.zeros_like(img)\n",
    "    else:\n",
    "        img, mask = DATASETS[dataset][N]\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def make_pred(model):\n",
    "        return to_numpy(model(img.view(1, *img.shape).cuda()).squeeze())\n",
    "    if N_model == 0:\n",
    "        pred, thr_mask = process_single_img(img, merge_by_mean, adaptive_thr, thr)\n",
    "#         preds = np.array([make_pred(model) for model in loaded_best_models])\n",
    "#         pred = np.mean(preds, axis=0)\n",
    "#         if merge_by_mean:\n",
    "#             if adaptive_thr:\n",
    "#                 thr_mask = process_adaptive_thr(pred)\n",
    "#             else:\n",
    "#                 thr_mask = (pred > thr).astype(np.uint8)\n",
    "#         else:\n",
    "#             if adaptive_thr:\n",
    "#                 thr_masks = [process_adaptive_thr(p) for p in preds]\n",
    "#             else:\n",
    "#                 thr_masks = (preds > thr).astype(np.uint8)\n",
    "#             thr_mask = np.median(thr_masks, axis=0).astype(np.uint8)\n",
    "    else:\n",
    "        pred = make_pred(loaded_best_models[N_model - 1])\n",
    "        thr_mask = (pred > thr).astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    img = to_numpy(img).swapaxes(0, 2)\n",
    "    img = np.clip((img * STD + MEAN), 0, 1)\n",
    "    mask = to_numpy(mask).swapaxes(0, 2)\n",
    "    mask[:, :, :2] = (mask[:, :, :2] + 1) * 0.5\n",
    "    score = jaccard_score(mask[:,:,2], thr_mask.T, average=\"micro\")\n",
    "    if mask[:,:, 2].sum() == 0 and thr_mask.sum() == 0:\n",
    "        score = 1\n",
    "    img_over = img.copy()\n",
    "    if overlay_pred:\n",
    "        img_over[thr_mask.T == 1] = [1, 0, 0]\n",
    "    if overlay_true:\n",
    "        img_over[mask[:, :, 2] == 1] = [0, 0, 1]\n",
    "        if overlay_pred:\n",
    "            img_over[(mask[:, :, 2] == 1) & (thr_mask.T == 1)] = [1, 0, 1]\n",
    "    pred = np.stack([pred.T] * 3, 2)\n",
    "    thr_mask = np.stack([thr_mask.T] * 3, 2)\n",
    "    if \"test\" in dataset:\n",
    "        stacked = np.hstack([img, img_over, pred])\n",
    "        plt.figure(figsize=(24,8))\n",
    "    else:\n",
    "        stacked = np.hstack([img_over, mask, pred, thr_mask])\n",
    "        plt.figure(figsize=(32,8))\n",
    "    plt.imshow(stacked, cmap=\"gray\")\n",
    "#     plt.title(f\"Jaccard={score:.3f}. Max value: {pred.max():.2f}\", fontdict={\"fontsize\": 25})\n",
    "    plt.axis(\"off\")\n",
    "    SINGLE_PRED = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:48.151354Z",
     "start_time": "2020-03-16T13:47:47.754004Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm data/preds/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T13:47:48.918472Z",
     "start_time": "2020-03-16T13:47:48.895175Z"
    }
   },
   "outputs": [],
   "source": [
    "empty_idx = set(np.load(\"empty_test_idx.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-16T15:05:22.956581Z",
     "start_time": "2020-03-16T13:48:21.665211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11481/11481 [1:17:01<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "PREDS_PATH = \"data/preds\"\n",
    "n_img = 0\n",
    "# max_preds = []\n",
    "for img, aug_imgs, idx in tqdm(test_dataset512):\n",
    "#     n_img += 1\n",
    "#     if n_img < 6:\n",
    "#         n_img += 1\n",
    "#         continue\n",
    "    if idx in empty_idx:\n",
    "        resized_thr_mask = np.zeros((1024, 1024), dtype=np.uint8)\n",
    "    else:\n",
    "        pred, thr_mask = process_single_img(aug_imgs, merge_by_mean=False, adaptive_thr=True, thr=0.5)\n",
    "    #     max_preds.append((pred.max(), np.percentile(pred, 95), idx))\n",
    "        resized_thr_mask = cv2.resize(thr_mask, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.imwrite(PREDS_PATH + \"/\" + (idx + \".tif\"), resized_thr_mask)\n",
    "#     if n_img > 10:\n",
    "#         break\n",
    "    \n",
    "# print(resized_thr_mask.max())\n",
    "# plt.imshow(resized_thr_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T11:05:49.897362Z",
     "start_time": "2020-03-12T11:05:49.867246Z"
    }
   },
   "outputs": [],
   "source": [
    "max_preds_values, max_preds_names = max_preds[:, :2].astype(float), max_preds[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T11:12:11.144594Z",
     "start_time": "2020-03-12T11:12:11.122184Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_by_name(name):\n",
    "    test_path = \"/home/zakirov/datasets/opencities/test\"\n",
    "    img = cv2.imread(f\"{test_path}/{name}/{name}.tif\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T11:20:30.344791Z",
     "start_time": "2020-03-12T11:20:30.015537Z"
    }
   },
   "source": [
    "show_by_name(\"d79626\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T11:20:12.675094Z",
     "start_time": "2020-03-12T11:20:12.644530Z"
    }
   },
   "source": [
    "#  (max_preds_values[:, 1] > 0.1)\n",
    "max_preds[(max_preds_values[:, 0] < 0.5) & (max_preds_values[:, 0] > 0.4) ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T11:21:44.939542Z",
     "start_time": "2020-03-12T11:21:44.919901Z"
    }
   },
   "source": [
    "np.save(\"empty_test_idx\", max_preds_names[max_preds_values[:, 0] < 0.3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit9890b48dadda4b3ab35703e845f6f232"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
