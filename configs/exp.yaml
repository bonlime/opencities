# train config
segm_arch: segm_bifpn
arch: efficientnet_b3
# model_params: {"decoder_merge_policy": "cat"}
# model_params: {"merge_policy": "cat", "num_fpn_layers": 3}
model_params: {"merge_policy": "cat", "num_fpn_layers": 3}
# model_params: {"aspp_dilation_rates": [2, 4, 6], "drop_rate": 0.1}
lookahead: True
optim: novograd
# lr: 1e-2
workers: 8
wd: 1e-4
size: 384
bs: 16
decoder_warmup_epochs: 10
# epochs: 100 # replaced by phases now
# datasets: [tier1, tier2, inria]
# datasets: [tier1, inria]
dataset: [tier1]
# phases: [{"ep":[0, 2], "lr": [0, 0.5]}, {"ep":[2, 6], "lr": [0.5, 0.01]}, {"ep":[6, 50], "lr": [ 0.01 , 0], mode: "cos"}]
phases: [{"ep":[0, 70], "lr": [ 0.01 , 0], mode: "cos"}]
opt_level: O1
criterion: [reduced_focal,  1]
# cutmix: True
augmentation: hard
dropout: 0.2
dropout_epochs: 30
# resume: logs/8.deeplab_reduced_focal_20200302_183355/model.chpn
# resume: logs/8.bifpn_reduced_focal_20200302_191335/model.chpn
# resume: logs/11.bifpn_2l_2dtst_seres101_20200304_233016/model.chpn
name: 12.bifpn_3l_1dtst_effnetb3